---
title: "Probing Information Salience (ACL'25)"
last_modified_at: 2025-07-31T12:20:02-05:00
classes: wide
categories:
  - publication
tags:
  - nlp
---

Jan Trienes presented research at [ACL 2025 in Vienna](https://2025.aclweb.org/) on how large language models (LLMs) identify important information in text.

Using text summarization as a probe, we discovered that while LLMs demonstrate a **consistent internal notion of salience**, their ability to reason about it remains **unreliable** and only **partially aligns with human judgment**. These findings have important implications for deploying LLMs in tasks like summarization, simplification, and retrieval-augmented generation (RAG).

We appreciated the engaging discussions at the conference and are excited to build on this work!

### Reference

<span class="cite-hidden">{% cite Trienes2025_acl_information-salience %}</span>
{% bibliography --cited --group_by none --template bib %}



