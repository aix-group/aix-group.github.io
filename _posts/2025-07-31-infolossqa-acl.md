---
title: "Probing Information Salience (ACL'25)"
last_modified_at: 2025-07-31T12:20:02-05:00
classes: wide
categories:
  - publication
tags:
  - nlp
---

Jan Trienes presented research at **[ACL 2025 in Vienna](https://2025.aclweb.org/)** on **how large language models (LLMs) identify important information** in text.

Using **text summarization as a probe**, we discovered that while LLMs demonstrate a **consistent internal notion of salience**, their ability to reason about it remains **unreliable** and only **partially aligns with human judgment**. These findings have important implications for deploying LLMs in tasks like summarization, simplification, and retrieval-augmented generation (RAG).

We appreciated the engaging discussions at the conference and are excited to build on this work!

<i class="fa fa-book-reader" style="font-size:12px;color: #7a46eb;"></i> {% reference Trienes2025_acl_information-salience %} [PDF](https://aclanthology.org/2025.findings-acl.1204.pdf) 

<i class="fa-brands fa-github" style="font-size:12px;color: #7a46eb;"></i>  [Source Code](https://github.com/jantrienes/llm-salience)


